{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyashukla/genai-class/blob/main/add-spacerag-example/examples/cookbooks/rag/spacerag/part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THzPCsny7yLl"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/weave/blob/anish/add-spacerag-example/examples/cookbooks/rag/spacerag/part2.ipynb)\n",
        "<!--- @wandbcode{weave-spacerag-cookbook} -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pao9QWqE7yLn"
      },
      "outputs": [],
      "source": [
        "IN_COLAB = False\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    import os\n",
        "    os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
        "    os.environ[\"TOGETHER_API_KEY\"] = userdata.get(\"TOGETHER_API_KEY\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XJNpOy777yLn",
        "outputId": "d4d65ec4-e140-4b8b-ad25-95a8192b77cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder 'weave_cookbooks' already exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "repo_url = \"https://github.com/wandb/weave.git\"\n",
        "target_folder = \"weave_cookbooks\"\n",
        "subdirectory = \"examples/cookbooks\"\n",
        "branch = \"anish/add-spacerag-example\"\n",
        "\n",
        "if not os.path.exists(target_folder) and IN_COLAB:\n",
        "    print(f\"Cloning repository: {repo_url}\")\n",
        "\n",
        "    # Clone the entire repository to a temporary folder\n",
        "    temp_folder = \"temp_weave_repo\"\n",
        "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \"--branch\", branch, repo_url, temp_folder], check=True)\n",
        "\n",
        "    # Move the desired subdirectory to the target folder\n",
        "    shutil.move(os.path.join(temp_folder, subdirectory), target_folder)\n",
        "\n",
        "    # Remove the temporary folder\n",
        "    shutil.rmtree(temp_folder)\n",
        "\n",
        "    print(f\"Successfully cloned {subdirectory} from branch '{branch}' to {target_folder}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Folder '{target_folder}' already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BPU5GWX27yLo",
        "outputId": "2fdad56d-9bb7-4806-f12c-f9468575135f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/weave_cookbooks/rag/spacerag\n",
            "Requirement already satisfied: weave in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.51.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.52.2)\n",
            "Requirement already satisfied: together in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.2.3)\n",
            "Requirement already satisfied: tonic-validate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (6.1.0)\n",
            "Requirement already satisfied: emoji>=2.12.1 in /usr/local/lib/python3.10/dist-packages (from weave->-r requirements.txt (line 1)) (2.14.0)\n",
            "Requirement already satisfied: gql[aiohttp,requests] in /usr/local/lib/python3.10/dist-packages (from weave->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from weave->-r requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from weave->-r requirements.txt (line 1)) (2.9.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from weave->-r requirements.txt (line 1)) (13.9.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/local/lib/python3.10/dist-packages (from weave->-r requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: uuid-utils>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from weave->-r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: wandb>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from weave->-r requirements.txt (line 1)) (0.18.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (3.10.10)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (8.1.7)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (3.16.1)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (17.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: typer<0.13,>=0.9 in /usr/local/lib/python3.10/dist-packages (from together->-r requirements.txt (line 5)) (0.12.5)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r requirements.txt (line 7)) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (6.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from ragas->-r requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from ragas->-r requirements.txt (line 8)) (0.3.7)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from ragas->-r requirements.txt (line 8)) (0.3.15)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas->-r requirements.txt (line 8)) (0.3.5)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (from ragas->-r requirements.txt (line 8)) (0.2.5)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas->-r requirements.txt (line 8)) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas->-r requirements.txt (line 8)) (1.4.4)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas->-r requirements.txt (line 8)) (0.3.4)\n",
            "Requirement already satisfied: aioboto3<13.0.0,>=12.4.0 in /usr/local/lib/python3.10/dist-packages (from tonic-validate->-r requirements.txt (line 9)) (12.4.0)\n",
            "Requirement already satisfied: google-generativeai<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from tonic-validate->-r requirements.txt (line 9)) (0.5.4)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.35.8 in /usr/local/lib/python3.10/dist-packages (from tonic-validate->-r requirements.txt (line 9)) (1.51.3)\n",
            "Requirement already satisfied: aiobotocore==2.12.3 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.12.3->aioboto3<13.0.0,>=12.4.0->tonic-validate->-r requirements.txt (line 9)) (2.12.3)\n",
            "Requirement already satisfied: botocore<1.34.70,>=1.34.41 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.12.3->aiobotocore[boto3]==2.12.3->aioboto3<13.0.0,>=12.4.0->tonic-validate->-r requirements.txt (line 9)) (1.34.69)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.12.3->aiobotocore[boto3]==2.12.3->aioboto3<13.0.0,>=12.4.0->tonic-validate->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.12.3->aiobotocore[boto3]==2.12.3->aioboto3<13.0.0,>=12.4.0->tonic-validate->-r requirements.txt (line 9)) (0.12.0)\n",
            "Requirement already satisfied: boto3<1.34.70,>=1.34.41 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.12.3->aioboto3<13.0.0,>=12.4.0->tonic-validate->-r requirements.txt (line 9)) (1.34.69)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together->-r requirements.txt (line 5)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together->-r requirements.txt (line 5)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together->-r requirements.txt (line 5)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together->-r requirements.txt (line 5)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together->-r requirements.txt (line 5)) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (1.25.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (0.14.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (4.23.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (0.19.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->weave->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->weave->-r requirements.txt (line 1)) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together->-r requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->weave->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->weave->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas->-r requirements.txt (line 8)) (2024.9.11)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13,>=0.9->together->-r requirements.txt (line 5)) (1.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.17.1->weave->-r requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.17.1->weave->-r requirements.txt (line 1)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.17.1->weave->-r requirements.txt (line 1)) (4.3.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.17.1->weave->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.17.1->weave->-r requirements.txt (line 1)) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.17.1->weave->-r requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.17.1->weave->-r requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.2 in /usr/local/lib/python3.10/dist-packages (from gql[aiohttp,requests]->weave->-r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from gql[aiohttp,requests]->weave->-r requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gql[aiohttp,requests]->weave->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas->-r requirements.txt (line 8)) (2.0.35)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas->-r requirements.txt (line 8)) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas->-r requirements.txt (line 8)) (0.1.137)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas->-r requirements.txt (line 8)) (1.33)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->-r requirements.txt (line 8)) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->-r requirements.txt (line 8)) (2.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 7)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 7)) (2024.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->-r requirements.txt (line 8)) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->-r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.17.1->weave->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave->-r requirements.txt (line 1)) (4.0.11)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (1.65.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (4.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas->-r requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.35.8->tonic-validate->-r requirements.txt (line 9)) (0.20.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas->-r requirements.txt (line 8)) (3.10.10)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->weave->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas->-r requirements.txt (line 8)) (3.1.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.9.3->together->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (4.1.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<1.34.70,>=1.34.41->aiobotocore[boto3]==2.12.3->aioboto3<13.0.0,>=12.4.0->tonic-validate->-r requirements.txt (line 9)) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<1.34.70,>=1.34.41->aiobotocore[boto3]==2.12.3->aioboto3<13.0.0,>=12.4.0->tonic-validate->-r requirements.txt (line 9)) (0.10.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave->-r requirements.txt (line 1)) (5.0.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->tonic-validate->-r requirements.txt (line 9)) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->-r requirements.txt (line 8)) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(target_folder) and IN_COLAB:\n",
        "    %cd weave_cookbooks/rag/spacerag\n",
        "    !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W38uw2oh7yLr"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "from weave import Evaluation\n",
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "from together import Together\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JjF12eEJ7yLr",
        "outputId": "41edeace-1fe7-496a-dbf1-f91c55a10c02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as Weights & Biases user: lavanyashukla.\n",
            "View Weave data at https://wandb.ai/lavanyashukla/space_rag_test1/weave\n"
          ]
        }
      ],
      "source": [
        "weave.init('space_rag_test1')\n",
        "\n",
        "# SERVE MODEL FROM TOGETHER ENDPOINT\n",
        "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hGbs2ryx7yLs"
      },
      "outputs": [],
      "source": [
        "# CHUNK DATA FROM EXTERNAL KNOWLEDGEBASE\n",
        "@weave.op\n",
        "def get_chunked_data(file):\n",
        "    # get data - file\n",
        "    with open(file, 'r') as file:\n",
        "        # Read the contents of the file into a variable\n",
        "        text = file.read()\n",
        "\n",
        "    # split doc into chunks\n",
        "    chunk_size = 2048 # 🛠️LEVER\n",
        "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)] # 🛠️LEVER\n",
        "    return chunks\n",
        "\n",
        "# EMBED DATA\n",
        "@weave.op\n",
        "def get_text_embedding(input):\n",
        "    api_key_openai = os.environ[\"OPENAI_API_KEY\"]\n",
        "    client = OpenAI(api_key=api_key_openai)\n",
        "\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\", # 🛠️LEVER\n",
        "        input=input\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# MAKE VECTORDB\n",
        "@weave.op\n",
        "def make_vector_db(file):\n",
        "    # get chunked data from function get_chunked_data()\n",
        "    chunks = get_chunked_data(file)\n",
        "    # embed data\n",
        "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
        "    # embed data into vectordb\n",
        "    d = text_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(text_embeddings)\n",
        "    return index, chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kRh3mP_L7yLs",
        "outputId": "16c2142c-2b77-4f8f-e6ab-4c3423c0432c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f3e8-d5fb-7e61-b157-f23da629bc07\n"
          ]
        }
      ],
      "source": [
        "file = './data/space.txt'\n",
        "index, chunks = make_vector_db(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f8RuuvYW7yLt"
      },
      "outputs": [],
      "source": [
        "# ANSWER QUESTION\n",
        "@weave.op\n",
        "def predict(model, prompt):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.5, # 🛠️LEVER\n",
        "        top_p=1, # 🛠️LEVER\n",
        "        max_tokens=1024,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    answer = []\n",
        "    for chunk in completion:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            answer.append(chunk.choices[0].delta.content)\n",
        "\n",
        "    result = ''.join(answer)\n",
        "    print(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4mLorY2M7yLt"
      },
      "outputs": [],
      "source": [
        "# RETRIEVE CHUNKS SIMILAR TO THE QUESTION\n",
        "@weave.op\n",
        "def retrieve_context(question: str) -> list:\n",
        "    question_embeddings = np.array([get_text_embedding(question)])\n",
        "    # Retrieve similar chunks from the vectorDB\n",
        "    D, I = index.search(question_embeddings, k=2) # 🛠️LEVER k=?\n",
        "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "    return retrieved_chunk\n",
        "\n",
        "class SpaceRAGModel(weave.Model):\n",
        "    model: str\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question: str):\n",
        "        retrieved_chunk = retrieve_context(question)\n",
        "        print(\"Question: \"+question)\n",
        "\n",
        "        # Combine context and question in a prompt # 🛠️LEVER\n",
        "        prompt = f\"\"\"\n",
        "        Use this context to answer the question, don't use any prior knowledge.\n",
        "        Be concise in your answers.\n",
        "        ---------------------\n",
        "        {retrieved_chunk}\n",
        "        ---------------------\n",
        "        Question: {question}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        answer = predict(self.model, prompt)\n",
        "        print(\"___________________________\")\n",
        "        return {'answer': answer, 'retrieved_chunk': retrieved_chunk}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PpO97J2T7yLt"
      },
      "outputs": [],
      "source": [
        "def string_to_dict(input_string):\n",
        "    # Use regular expressions to find all JSON-like objects in the string\n",
        "    json_objects = re.findall(r'\\{.*?\\}', input_string)\n",
        "\n",
        "    # Initialize an empty dictionary to store the combined results\n",
        "    combined_dict = {}\n",
        "\n",
        "    for obj in json_objects:\n",
        "        try:\n",
        "            # Parse each JSON object\n",
        "            parsed_dict = json.loads(obj)\n",
        "            # Update the combined dictionary with the parsed data\n",
        "            combined_dict.update(parsed_dict)\n",
        "        except (ValueError, json.JSONDecodeError) as e:\n",
        "            print(f\"Error processing part: {obj}\\nError: {e}\")\n",
        "\n",
        "    return combined_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KWA1mI3R7yLu",
        "outputId": "fc2e275a-66b3-41fd-a21c-9d29b7950f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:weave.trace_server_bindings.remote_http_trace_server:retry_attempt\n"
          ]
        }
      ],
      "source": [
        "dataset_ref = weave.ref(\"weave:///lavanyashukla/spacedata/object/space_dataset_llm_comprehensive:VBd5Ys7b3hGFmJJqdGTATVQYgKKrg70EiNV5FdpwFxs\").get()\n",
        "small_questions = dataset_ref.rows[:2] # TODO: CHANGE TO 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8Ou5EOu07yLu"
      },
      "outputs": [],
      "source": [
        "def replace_nan_in_dict(result):\n",
        "    for key in result:\n",
        "        if isinstance(result[key], float) and np.isnan(result[key]):\n",
        "            result[key] = 0\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h2K7y4jY7yLu"
      },
      "outputs": [],
      "source": [
        "# Evaluate with an LLM\n",
        "@weave.op\n",
        "def llm_judge_scorer(ground_truth: str, model_output: dict) -> dict:\n",
        "    scorer_llm = \"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\"\n",
        "    answer = model_output['answer']\n",
        "    retrieved_chunk = model_output['retrieved_chunk']\n",
        "\n",
        "    eval_rubrics = [\n",
        "    {\n",
        "        \"metric\": \"concise\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        false: The answer is long and difficult to understand.\n",
        "        true: The answer is completely concise, readable and engaging.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"metric\": \"relevant\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        false: The answer is not relevant to the original text, or has significant flaws.\n",
        "        true: The answer is completely relevant to the original text, and provides additional value or insight.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"metric\": \"🥇accurate\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        Compare the factual content of the model's answer with the correct answer. Ignore any differences in style, grammar, or punctuation.\n",
        "        false: There is a disagreement between the model's answer and the correct answer.\n",
        "        true: The model's answer contains all the same details as the correct answer.\n",
        "        \"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "    scoring_prompt = \"\"\"\n",
        "    You have the correct answer, original text and the model's answer below.\n",
        "    Based on the specified evaluation metric and rubric, assign a true or false score the summary.\n",
        "    Then, return a JSON object with the metric name as the key and the evaluation score (false or true) as the value. Don't output anything else.\n",
        "\n",
        "    # Evaluation metric:\n",
        "    {metric}\n",
        "\n",
        "    # Evaluation rubrics:\n",
        "    {rubrics}\n",
        "\n",
        "    # Correct Answer\n",
        "    {ground_truth}\n",
        "\n",
        "    # Original Text\n",
        "    {retrieved_chunk}\n",
        "\n",
        "    # Model Answer\n",
        "    {model_answer}\n",
        "\n",
        "    \"\"\"\n",
        "    evals = \"\"\n",
        "    for i in eval_rubrics:\n",
        "        eval_output = predict(scorer_llm,\n",
        "            scoring_prompt.format(\n",
        "                ground_truth=ground_truth, retrieved_chunk=retrieved_chunk, model_answer=answer,\n",
        "                metric=i[\"metric\"], rubrics=i[\"rubrics\"]\n",
        "            ))+\" \"\n",
        "        evals+=eval_output\n",
        "    # evals_json = format_string_to_json(evals)\n",
        "    evals_dict = string_to_dict(evals)\n",
        "    # print(\"___________________________\")\n",
        "    # print(evals_dict)\n",
        "    # print(\"___________________________\")\n",
        "    return evals_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y4cRBlJt7yLu"
      },
      "outputs": [],
      "source": [
        "# def ragas_score(question, ground_truth, model_output):\n",
        "#     from datasets import Dataset\n",
        "#     from ragas import evaluate\n",
        "#     from ragas.metrics import faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision\n",
        "\n",
        "#     metric_modules = [\n",
        "#         answer_relevancy,\n",
        "#         context_recall,\n",
        "#     ]\n",
        "\n",
        "#     # Convert the retrieved_chunk to a list of strings\n",
        "#     contexts = [str(chunk) for chunk in model_output[\"retrieved_chunk\"]]\n",
        "\n",
        "#     qa_dataset = Dataset.from_dict(\n",
        "#         {\n",
        "#             \"question\": [question],\n",
        "#             \"ground_truth\": [ground_truth],\n",
        "#             \"answer\": [model_output[\"answer\"]],\n",
        "#             \"contexts\": [contexts],  # Wrap contexts in another list\n",
        "#         }\n",
        "#     )\n",
        "#     result = evaluate(qa_dataset, metrics=metric_modules,\n",
        "#                       raise_exceptions=False)\n",
        "#     return replace_nan_in_dict(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aoqWszGw7yLu"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def tonic_validate_score(question: str, ground_truth: str, model_output: dict) -> dict:\n",
        "    from tonic_validate import Benchmark, ValidateScorer\n",
        "    from tonic_validate.metrics import DuplicationMetric, AnswerSimilarityMetric, AnswerConsistencyMetric\n",
        "\n",
        "    metric_modules = [DuplicationMetric(), AnswerSimilarityMetric(), AnswerConsistencyMetric()]\n",
        "\n",
        "    def get_llm_response(question):\n",
        "        return {\n",
        "            \"llm_answer\": model_output['answer'],\n",
        "            \"llm_context_list\": (\n",
        "                [model_output['retrieved_chunk']]\n",
        "                if isinstance(model_output['retrieved_chunk'], str)\n",
        "                else model_output['retrieved_chunk']\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    benchmark = Benchmark(questions=[question], answers=[ground_truth])\n",
        "    scorer = ValidateScorer(metrics=metric_modules)\n",
        "    run = scorer.score(benchmark, get_llm_response)\n",
        "    return run.run_data[0].scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8LnvJEcy7yLu",
        "outputId": "b89bb361-70e6-4d57-9bd8-a7489004e93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Model: meta-llama/Meta-Llama-3-70B-Instruct-Turbo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How is the sense of spaciousness addressed in the design of habitats for the colony?\n",
            "The sense of spaciousness is addressed by providing: \n",
            "\n",
            "1. Large and small, private and community spaces.\n",
            "2. Long vistas and short ones.\n",
            "3. Flexible, manipulatable architecture that allows colonists to reshape the interior.\n",
            "4. Large-scale vistas by making the habitat large enough to lessen the sense of artificiality.\n",
            "5. Panoramic views of the Earth, Moon, and stars.\n",
            "6. Access to regions of zero gravity.\n",
            "7. Live, growing things such as vegetation and animals.\n",
            "8. Random variation in climate (optional).\n",
            "___________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the steps involved in the process of producing aluminum from lunar soil, specifically in regards to glass grinding and acid leaching?\n",
            "The steps involved in producing aluminum from lunar soil, specifically in regards to glass grinding and acid leaching, are:\n",
            "\n",
            "1. The lunar soil is melted in a solar furnace at a temperature of 2000 K, then quenched in water to form a glass.\n",
            "2. The glass is ground to 65 mesh.\n",
            "3. The ground glass is leached with sulfuric acid.\n",
            "___________________________\n",
            "{\"concise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n",
            "{\"concise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving responses: 100%|██████████| 1/1 [00:00<00:00, 696.38it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f413-f968-7c22-a70f-74d354da8c99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f413-fc51-73c0-b38c-f9394ba7556b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f413-fea5-75b0-b6c7-38c3e2181603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-10a6-7ac3-9eec-eff87ef8a873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-1309-7cf0-8e07-9526f2562f5b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-14fe-7a32-992d-ede7f6d8c8dd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-175b-73a3-a082-b4520898681d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-1a92-7a30-be29-6a89f1e861cb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-250f-7613-b638-f047ccd05abc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-2765-7c12-8537-678e56c8f7b8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|██████████| 1/1 [00:13<00:00, 13.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-2a0f-7ff0-a1cf-af61a6b5352b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving responses: 100%|██████████| 1/1 [00:00<00:00, 6168.09it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-3191-70e1-8458-e843b26ab1c5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-3358-78d2-b6f3-84805510862e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-354b-7221-9470-e24ffee927bb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-3e50-7cd1-9614-717949bbe72c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-40af-7d41-acd6-025df3f5a5e3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|██████████| 1/1 [00:04<00:00,  4.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-425c-7052-9d4a-897c413ea7ab\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation summary\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'llm_judge_scorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'concise'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'accurate'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'relevant'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'tonic_validate_score'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'answer_consistency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'answer_similarity'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'duplication_metric'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m5.03689181804657\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'llm_judge_scorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'concise'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'accurate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'relevant'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'tonic_validate_score'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer_consistency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer_similarity'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'duplication_metric'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.03689181804657</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f413-d578-7f43-86e8-124622dda5ab\n",
            "RAG Model: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How is the sense of spaciousness addressed in the design of habitats for the colony?\n",
            "The sense of spaciousness is addressed in the design of habitats for the colony by providing a range of options for colonists, including large and small, private and community spaces, and by allowing for flexible and manipulatable architecture.\n",
            "___________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the steps involved in the process of producing aluminum from lunar soil, specifically in regards to glass grinding and acid leaching?\n",
            "The steps involved in producing aluminum from lunar soil are:\n",
            "1. Melting the lunar soil in a solar furnace at 2000 K.\n",
            "2. Quenching the molten material in water to form a glass.\n",
            "3. Grinding the glass to 65 mesh.\n",
            "4. Leaching the ground glass with sulfuric acid.\n",
            "___________________________\n",
            "{\"concise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n",
            "{\"concise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving responses: 100%|██████████| 1/1 [00:00<00:00, 5949.37it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-5fca-7131-92cb-ed5dbe79e007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-6450-7a72-ae7f-be656532357e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-6605-74a3-94f8-a2e8c38644d4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-6e79-71f3-aacf-ddb2e0539f4f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-7081-7d83-9c72-5c9d7735fb62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|██████████| 1/1 [00:05<00:00,  5.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-72ad-7d52-80dc-86500c4908b8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving responses: 100%|██████████| 1/1 [00:00<00:00, 7898.88it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-79a5-7891-ad12-968caa4f3a27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-7b71-7492-b7c6-c74e7435a0fc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-7d38-7a70-9ffd-4a7f6d965067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-88cc-7bc3-8752-536d3db13c98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-8b3f-7360-97eb-907731d94705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-8d87-73e0-b36e-fb1309a1a853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-8fdf-7603-8f76-093306db5df9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation summary\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'llm_judge_scorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'concise'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'accurate'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'relevant'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'tonic_validate_score'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'answer_consistency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'answer_similarity'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m2.5\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'duplication_metric'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m2.6216472387313843\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'llm_judge_scorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'concise'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'accurate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'relevant'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'tonic_validate_score'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer_consistency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer_similarity'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'duplication_metric'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.6216472387313843</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-46d9-7540-95fa-3faed78e8488\n",
            "RAG Model: Snowflake/snowflake-arctic-instruct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:together:error_code=model_not_available error_message='Unable to access non-serverless model Snowflake/snowflake-arctic-instruct. Please visit https://api.together.ai/models/Snowflake/snowflake-arctic-instruct to create and start a new dedicated endpoint for the model.' error_param=None error_type=invalid_request_error message='Together API error received' stream_error=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How is the sense of spaciousness addressed in the design of habitats for the colony?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_output failed\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 205, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 57, in async_call_op\n",
            "    call_res = func.call(*args, __should_raise=True, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 385, in call\n",
            "    return _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 441, in _do_call\n",
            "    execute_result = _execute_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 350, in _execute_call\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 348, in _execute_call\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"<ipython-input-9-a4f48d763d24>\", line 28, in predict\n",
            "    answer = predict(self.model, prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 623, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 441, in _do_call\n",
            "    execute_result = _execute_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 350, in _execute_call\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 348, in _execute_call\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"<ipython-input-8-a40d93bf452b>\", line 4, in predict\n",
            "    completion = client.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/together/resources/chat/completions.py\", line 141, in create\n",
            "    response, _, _ = requestor.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/together/abstract/api_requestor.py\", line 249, in request\n",
            "    resp, got_stream = self._interpret_response(result, stream)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/together/abstract/api_requestor.py\", line 632, in _interpret_response\n",
            "    self._interpret_response_line(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/together/abstract/api_requestor.py\", line 701, in _interpret_response_line\n",
            "    raise self.handle_error_response(resp, rcode, stream_error=stream)\n",
            "together.error.InvalidRequestError: Error code: 400 - {\"message\": \"Unable to access non-serverless model Snowflake/snowflake-arctic-instruct. Please visit https://api.together.ai/models/Snowflake/snowflake-arctic-instruct to create and start a new dedicated endpoint for the model.\", \"type_\": \"invalid_request_error\", \"code\": \"model_not_available\"}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:together:error_code=model_not_available error_message='Unable to access non-serverless model Snowflake/snowflake-arctic-instruct. Please visit https://api.together.ai/models/Snowflake/snowflake-arctic-instruct to create and start a new dedicated endpoint for the model.' error_param=None error_type=invalid_request_error message='Together API error received' stream_error=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the steps involved in the process of producing aluminum from lunar soil, specifically in regards to glass grinding and acid leaching?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_output failed\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 205, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 57, in async_call_op\n",
            "    call_res = func.call(*args, __should_raise=True, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 385, in call\n",
            "    return _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 441, in _do_call\n",
            "    execute_result = _execute_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 350, in _execute_call\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 348, in _execute_call\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"<ipython-input-9-a4f48d763d24>\", line 28, in predict\n",
            "    answer = predict(self.model, prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 623, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 441, in _do_call\n",
            "    execute_result = _execute_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 350, in _execute_call\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 348, in _execute_call\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"<ipython-input-8-a40d93bf452b>\", line 4, in predict\n",
            "    completion = client.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/together/resources/chat/completions.py\", line 141, in create\n",
            "    response, _, _ = requestor.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/together/abstract/api_requestor.py\", line 249, in request\n",
            "    resp, got_stream = self._interpret_response(result, stream)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/together/abstract/api_requestor.py\", line 632, in _interpret_response\n",
            "    self._interpret_response_line(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/together/abstract/api_requestor.py\", line 701, in _interpret_response_line\n",
            "    raise self.handle_error_response(resp, rcode, stream_error=stream)\n",
            "together.error.InvalidRequestError: Error code: 400 - {\"message\": \"Unable to access non-serverless model Snowflake/snowflake-arctic-instruct. Please visit https://api.together.ai/models/Snowflake/snowflake-arctic-instruct to create and start a new dedicated endpoint for the model.\", \"type_\": \"invalid_request_error\", \"code\": \"model_not_available\"}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Predict and score failed\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 454, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 615, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 494, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 341, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 339, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 379, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 623, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 441, in _do_call\n",
            "    execute_result = _execute_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 350, in _execute_call\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 348, in _execute_call\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"<ipython-input-13-eb65784b6d84>\", line 5, in llm_judge_scorer\n",
            "    answer = model_output['answer']\n",
            "TypeError: 'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Predict and score failed\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 454, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 615, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 494, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 341, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 339, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 379, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 623, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 441, in _do_call\n",
            "    execute_result = _execute_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 350, in _execute_call\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 348, in _execute_call\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"<ipython-input-13-eb65784b6d84>\", line 5, in llm_judge_scorer\n",
            "    answer = model_output['answer']\n",
            "TypeError: 'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation summary\n",
              "\u001b[1m{\u001b[0m\u001b[32m'llm_judge_scorer'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'tonic_validate_score'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
              "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'llm_judge_scorer'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tonic_validate_score'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-9476-77f0-b47b-c3ad1c765531\n",
            "RAG Model: mistralai/Mixtral-8x22B-Instruct-v0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How is the sense of spaciousness addressed in the design of habitats for the colony?\n",
            " The sense of spaciousness is addressed by providing large-scale vistas, making the habitat large enough to lessen the sense of artificiality. Additionally, some parts of the structure are designed to be out of sight of others to limit a colonist's view. The design also includes access to regions of zero gravity and views of the Earth, the Moon, and stars. On a smaller scale, the artificiality of the interior is reduced by the presence of live, growing things such as vegetation, children playing, and animals. The design aims to have an environment that is not completely regimented, with some random variation in climate.\n",
            "___________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the steps involved in the process of producing aluminum from lunar soil, specifically in regards to glass grinding and acid leaching?\n",
            " The process involves melting lunar soil in a solar furnace at 2000 K and then quenching it in water to form a glass. The glass is then separated in a centrifuge and the steam condensed in radiators. The glass is ground to 65 mesh and leached with sulfuric acid. The pregnant solution containing aluminum sulfate is then separated from the waste material in a centrifuge.\n",
            "___________________________\n",
            "{\"concise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n",
            "{\"concise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving responses: 100%|██████████| 1/1 [00:00<00:00, 719.68it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-b1d4-7612-854a-55d1ad141fea\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-b3fb-72e3-8b4b-558aa11489c3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-b5a4-7ff3-b314-c057023e383b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-cddd-7023-94be-16bd1b5ea838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-d46f-7d22-843e-09d393749e2e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-d697-7160-9ea7-a573b304b265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-d8bc-7721-ba6f-c1fc8b0b3179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|██████████| 1/1 [00:11<00:00, 11.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-db26-73e3-b636-c88b910e3eac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving responses: 100%|██████████| 1/1 [00:00<00:00, 714.17it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-e1f7-7d03-816e-c7c0582b23a4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-e3f1-77c1-8528-302b665a6cef\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-e5c2-7ad0-8410-bb8c86a9db61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-f2d7-7400-93e8-2f063c31c6f0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-f53e-7c83-9641-1872e7a86fd1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-f72b-7090-9e20-2832679fc4cd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-f998-7f33-95ba-0a139919f47d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-fbb0-70f0-8035-ee6d115a9539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-fdd1-7903-9378-422cfd70a66b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|██████████| 1/1 [00:08<00:00,  8.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f415-004e-7ab3-9a01-f53c85f7ec5e\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation summary\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'llm_judge_scorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'concise'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'accurate'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'relevant'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'tonic_validate_score'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'answer_consistency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'answer_similarity'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.0\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'duplication_metric'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.387569785118103\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'llm_judge_scorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'concise'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'accurate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'relevant'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'tonic_validate_score'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer_consistency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer_similarity'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'duplication_metric'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.387569785118103</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🍩 https://wandb.ai/lavanyashukla/space_rag_test1/r/call/0192f414-97c6-7363-bbf8-9176c380c7dd\n"
          ]
        }
      ],
      "source": [
        "models = [\"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n",
        "          \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "          \"Snowflake/snowflake-arctic-instruct\",\n",
        "          \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
        "          ]  # 🛠️LEVER\n",
        "for model in models:\n",
        "    rag_model = SpaceRAGModel(model=model)\n",
        "    model_name = model.split('/')[-1]  # Get only the part after the '/'\n",
        "    evaluation = Evaluation(name=f\"spacerag-{model_name}\", dataset=small_questions, scorers=[\n",
        "    llm_judge_scorer,\n",
        "    # ragas_score,\n",
        "    tonic_validate_score\n",
        "])\n",
        "    print(f\"RAG Model: {model}\")\n",
        "    await evaluation.evaluate(rag_model, __weave={\"display_name\": f\"spacerag-{model_name}\"})\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bLGtl-vM7yLu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}